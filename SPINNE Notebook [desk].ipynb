{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seated Posture assessment using Images and Neural NEtworks (SPINNE)\n",
    "\n",
    "<img src=\"Logo.png\" style=\"width: 500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desk posture classifier\n",
    "\n",
    "For use with Jupyter Notebooks or Google Colab. The cells to load the dataset will not work as the dataset is not publicly available. ***Please see the demo notebook for an interactive webcam posture classifier demo.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig1.png\">\n",
    "\n",
    "\n",
    "                    Figure 1: Desk dataset; A - good, B - bad back, C - bad neck, D - bad slouch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# libraries for webcam use\n",
    "import cv2\n",
    "from IPython import display\n",
    "\n",
    "# CUDA config\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "class_labels = {\n",
    "    0 : \"good\",\n",
    "    1 : \"bad - back\",\n",
    "    2 : \"bad - neck\",\n",
    "    3 : \"bad - slouch\"\n",
    "}\n",
    "\n",
    "def imshow(im,label):\n",
    "    plt.imshow(im.numpy().transpose((1,2,0))) # pytorch tensors for RGB images are 3xWxH\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    if(label != -1):\n",
    "        class_label = class_labels[int(label)]\n",
    "        if(class_label == 'good'):\n",
    "            print('\\033[92m' + class_label + '\\033[0m')\n",
    "        else:\n",
    "            print('\\033[91m' + class_label + '\\033[0m')\n",
    "            \n",
    "def classify_dataset(model,dataloader):\n",
    "    # this loop classifies the data and returns an overall accuracy when compared with the ground truth\n",
    "    n_images = len(dataloader.dataset)\n",
    "\n",
    "    # vector to store predictions for each image in test set\n",
    "    predictions = []\n",
    "\n",
    "    # keep track of predictions which match actual label (groudn truth)\n",
    "    n_matching_predictions = 0\n",
    "\n",
    "    # loop over all batches in the dataset\n",
    "    for images,labels in dataloader:\n",
    "        # get output from current network\n",
    "        if torch.cuda.is_available():\n",
    "            output = model(images.cuda())\n",
    "        else:\n",
    "            output = model(images)    \n",
    "\n",
    "        ps = torch.exp(output)\n",
    "\n",
    "        max_val,max_idx = ps.max(1)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            predictions.append(max_idx[i])\n",
    "            if(max_idx[i] == labels[i]):\n",
    "                n_matching_predictions += 1\n",
    "                \n",
    "    classification_accuracy = n_matching_predictions / n_images\n",
    "    \n",
    "    return predictions,classification_accuracy\n",
    "\n",
    "def get_labels(model,dataloader):\n",
    "    # this loop get the target labels and predicted labels - assumes a batch size of 1\n",
    "\n",
    "    # vector to store target labels for each image in test set\n",
    "    targets = []\n",
    "    # vector to store predictions for each image in test set\n",
    "    predictions = []\n",
    "\n",
    "    # loop over all batches in the dataset\n",
    "    for images,labels in dataloader:\n",
    "        # get output from current network\n",
    "        if torch.cuda.is_available():\n",
    "            output = model(images.cuda())\n",
    "        else:\n",
    "            output = model(images)    \n",
    "\n",
    "        ps = torch.exp(output)    \n",
    "\n",
    "        predictions.append(output.argmax(dim=1))\n",
    "        if torch.cuda.is_available():\n",
    "            targets.append(labels.cuda())        \n",
    "        else:\n",
    "            targets.append(labels)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        predictions[i] = predictions[i].to(device='cpu')\n",
    "        targets[i] = targets[i].to(device='cpu')\n",
    "            \n",
    "    return targets, predictions\n",
    "\n",
    "# this is from https://deeplizard.com/learn/video/0LhiS6yu2qQ\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    classes = ['good','bad - back','bad - neck','bad - slouch']\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load dataset (please skip this section if you don't have the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Mount Google Drive and inflate files - for use with Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/My Drive/Colab Notebooks/Data/TRAIN/TRAIN.zip\" -d \"/content\"\n",
    "!unzip \"/content/drive/My Drive/Colab Notebooks/Data/VAL/VAL.zip\" -d \"/content\"\n",
    "!unzip \"/content/drive/My Drive/Colab Notebooks/Data/TEST/TEST.zip\" -d \"/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Load dataset transforms - unique for each dataset participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in training and testing data\n",
    "\n",
    "# using rescale/position/rotation variance in TRAINING DATA\n",
    "\n",
    "# for image sets 1,2,5,6 (default image sets other transforms are roughly based on - for normalisation purposes)\n",
    "# - we still provide reasonable augmentation to all the data so that the system is generalisable\n",
    "ryan_transform_1 = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.RandomAffine(degrees=0,translate=(0.15,0.1),scale=(0.85,1.15)), # translate - horizontal, vertical; scale - zoom out, zoom in\n",
    "                                         transforms.ColorJitter(brightness=(0.85,1.15),contrast=(0.85,1.15)), # colour augmentation\n",
    "                                         transforms.Resize(128), # height of resized image\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "# for image sets 3,4 (very close to camera, slightly dark - higher brightness more likely in transform)\n",
    "ryan_transform_2 = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.RandomAffine(degrees=0,translate=(0.15,0.1),scale=(0.7,0.85)), # translate - horizontal, vertical; scale - zoom out, zoom in\n",
    "                                         transforms.ColorJitter(brightness=(0.9,1.25),contrast=(0.85,1.15)), # colour augmentation\n",
    "                                         transforms.Resize(128), # height of resized image\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "# for image sets 9,10 (slightly further away from camera, slightly dark - higher brightness more likely in transform)\n",
    "ross_transform_1 = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.RandomAffine(degrees=0,translate=(0.15,0.1),scale=(1.0,1.25)), # translate - horizontal, vertical; scale - zoom out, zoom in\n",
    "                                         transforms.ColorJitter(brightness=(0.9,1.25),contrast=(0.85,1.15)), # colour augmentation\n",
    "                                         transforms.Resize(128), # height of resized image\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "# for image set 11 (quite close to camera, slightly dark - higher brightness more likely in transform)\n",
    "nick_transform_1 = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.RandomAffine(degrees=0,translate=(0.15,0.1),scale=(0.8,1.0)), # translate - horizontal, vertical; scale - zoom out, zoom in\n",
    "                                         transforms.ColorJitter(brightness=(0.9,1.25),contrast=(0.85,1.15)), # colour augmentation\n",
    "                                         transforms.Resize(128), # height of resized image\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "# for image set 7 (slightly close to camera)\n",
    "shaun_transform_1 = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.RandomAffine(degrees=0,translate=(0.15,0.1),scale=(0.8,1.05)), # translate - horizontal, vertical; scale - zoom out, zoom in\n",
    "                                         transforms.ColorJitter(brightness=(0.85,1.15),contrast=(0.85,1.15)), # colour augmentation\n",
    "                                         transforms.Resize(128), # height of resized image\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "# for image set 8 (slightly low down)\n",
    "mum_transform_1 = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.RandomAffine(degrees=0,translate=(0.15,0),scale=(0.85,1.15)), # translate - horizontal, vertical; scale - zoom out, zoom in\n",
    "                                         transforms.ColorJitter(brightness=(0.85,1.15),contrast=(0.85,1.15)), # colour augmentation\n",
    "                                         transforms.Resize(128), # height of resized image\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "# original basic transform\n",
    "transform = transforms.Compose([transforms.Resize(128), # height of resized image\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Load dataset (Jupyter Notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL (from local harddrive - Jupyter Notebooks)\n",
    "train_dataset_1 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/1', transform = ryan_transform_1)\n",
    "train_dataset_2 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/2', transform = ryan_transform_1)\n",
    "train_dataset_3 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/3', transform = ryan_transform_2)\n",
    "train_dataset_4 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/4', transform = ryan_transform_2)\n",
    "train_dataset_5 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/5', transform = ryan_transform_1)\n",
    "train_dataset_6 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/6', transform = ryan_transform_1)\n",
    "train_dataset_7 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/7', transform = nick_transform_1)\n",
    "train_dataset_8 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/8', transform = ross_transform_1)\n",
    "train_dataset_9 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TRAIN/9', transform = ross_transform_1)\n",
    "\n",
    "val_dataset_1 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/1', transform = ryan_transform_1)\n",
    "val_dataset_2 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/2', transform = ryan_transform_1)\n",
    "val_dataset_3 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/3', transform = ryan_transform_2)\n",
    "val_dataset_4 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/4', transform = ryan_transform_2)\n",
    "val_dataset_5 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/5', transform = ryan_transform_1)\n",
    "val_dataset_6 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/6', transform = ryan_transform_1)\n",
    "val_dataset_7 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/7', transform = nick_transform_1)\n",
    "val_dataset_8 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/8', transform = ross_transform_1)\n",
    "val_dataset_9 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/VAL/9', transform = ross_transform_1)\n",
    "\n",
    "test_dataset_1 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TEST/1', transform = shaun_transform_1)\n",
    "test_dataset_2 = datasets.ImageFolder('C:/Users/ryan_/Documents/University/EE581/Project/Data/Office/TEST/2', transform = mum_transform_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Load dataset (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLINE (from Google Drive as zip archives - Google Colab)\n",
    "train_dataset_1 = datasets.ImageFolder('/content/TRAIN/1', transform = ryan_transform_1)\n",
    "train_dataset_2 = datasets.ImageFolder('/content/TRAIN/2', transform = ryan_transform_1)\n",
    "train_dataset_3 = datasets.ImageFolder('/content/TRAIN/3', transform = ryan_transform_2)\n",
    "train_dataset_4 = datasets.ImageFolder('/content/TRAIN/4', transform = ryan_transform_2)\n",
    "train_dataset_5 = datasets.ImageFolder('/content/TRAIN/5', transform = ryan_transform_1)\n",
    "train_dataset_6 = datasets.ImageFolder('/content/TRAIN/6', transform = ryan_transform_1)\n",
    "train_dataset_7 = datasets.ImageFolder('/content/TRAIN/7', transform = nick_transform_1)\n",
    "train_dataset_8 = datasets.ImageFolder('/content/TRAIN/8', transform = ross_transform_1)\n",
    "train_dataset_9 = datasets.ImageFolder('/content/TRAIN/9', transform = ross_transform_1)\n",
    "\n",
    "val_dataset_1 = datasets.ImageFolder('/content/VAL/1', transform = ryan_transform_1)\n",
    "val_dataset_2 = datasets.ImageFolder('/content/VAL/2', transform = ryan_transform_1)\n",
    "val_dataset_3 = datasets.ImageFolder('/content/VAL/3', transform = ryan_transform_2)\n",
    "val_dataset_4 = datasets.ImageFolder('/content/VAL/4', transform = ryan_transform_2)\n",
    "val_dataset_5 = datasets.ImageFolder('/content/VAL/5', transform = ryan_transform_1)\n",
    "val_dataset_6 = datasets.ImageFolder('/content/VAL/6', transform = ryan_transform_1)\n",
    "val_dataset_7 = datasets.ImageFolder('/content/VAL/7', transform = nick_transform_1)\n",
    "val_dataset_8 = datasets.ImageFolder('/content/VAL/8', transform = ross_transform_1)\n",
    "val_dataset_9 = datasets.ImageFolder('/content/VAL/9', transform = ross_transform_1)\n",
    "## 2.4) Load dataset (Google Colab)\n",
    "test_dataset_1 = datasets.ImageFolder('/content/TEST/1', transform = shaun_transform_1)\n",
    "test_dataset_2 = datasets.ImageFolder('/content/TEST/2', transform = mum_transform_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) Concatenate seperate participant datasets (with different transforms) into single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate datasets\n",
    "train_dataset = torch.utils.data.ConcatDataset([train_dataset_1,\n",
    "                                               train_dataset_2,\n",
    "                                               train_dataset_3,\n",
    "                                               train_dataset_4,\n",
    "                                               train_dataset_5,\n",
    "                                               train_dataset_6,\n",
    "                                               train_dataset_7,\n",
    "                                               train_dataset_8,\n",
    "                                               train_dataset_9])\n",
    "\n",
    "val_dataset = torch.utils.data.ConcatDataset([val_dataset_1,\n",
    "                                             val_dataset_2,\n",
    "                                             val_dataset_3,\n",
    "                                             val_dataset_4,\n",
    "                                             val_dataset_5,\n",
    "                                             val_dataset_6,\n",
    "                                             val_dataset_7,\n",
    "                                             val_dataset_8,\n",
    "                                             val_dataset_9])\n",
    "\n",
    "test_dataset = torch.utils.data.ConcatDataset([test_dataset_1,\n",
    "                                              test_dataset_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6) Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7) Display some images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images,labels = next(iter(train_dataloader))\n",
    "imshow(images[0],labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Load classifier CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Define SpinNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 (with dropout and two hidden layers with activations in fcn)\n",
    "\n",
    "class SpinNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # feature extraction (convolution and pooling)\n",
    "        self.conv_1 = nn.Conv2d(3,16,kernel_size=(3,3),padding=(1,1))\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(3,3), stride=3)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(16,32,kernel_size=(3,3),padding=(1,1))\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(3,3), stride=3)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(32,16,kernel_size=(3,3),padding=(1,1))\n",
    "        self.pool_3 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        \n",
    "        # classifier (fully connected network)\n",
    "        self.hidden_1 = nn.Linear(1344,256)\n",
    "        self.hidden_2 = nn.Linear(256,256)\n",
    "        self.output_layer = nn.Linear(256,4)\n",
    "        self.classification_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.pool_1(x)\n",
    "        \n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool_2(x)\n",
    "        \n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = self.pool_3(x)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.hidden_1(x.view(-1,1344))))\n",
    "        x = self.dropout(F.relu(self.hidden_2(x)))\n",
    "        x = self.output_layer(x)\n",
    "        x = self.classification_fn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Load SpinNet model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is selecting the custom model\n",
    "model = SpinNET()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Download pretrained ILSVRC model for transfer learning (download may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is selecting the pretrained model\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) modify ILSVRC network to have 4 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = list(model.classifier.children())\n",
    "\n",
    "# # change last linear layer to correct number of output classes and add a log softmax classifier\n",
    "# mod.pop()\n",
    "# mod.append(nn.Linear(4096,4))\n",
    "# mod.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "# model.classifier = torch.nn.Sequential(*mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# define loss function and optimiser type/parameters\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(),lr=0.0001)\n",
    "#optimiser = optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "model.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for images,labels in train_dataloader:   \n",
    "        # clear gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # get output from current network\n",
    "        if torch.cuda.is_available():\n",
    "            output = model(images.cuda())\n",
    "        else:\n",
    "            output = model(images)\n",
    "        \n",
    "        # calculate loss for this epoch - cross-entropy loss (log softmax plus negative log likelihood)\n",
    "        if torch.cuda.is_available():\n",
    "            loss = criterion(output.cuda(),labels.cuda())\n",
    "        else:\n",
    "            loss = criterion(output,labels)\n",
    "        \n",
    "        # back propagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimisation step - update weights\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"EPOCH \", str(e+1))\n",
    "        print(f\"Training loss: {running_loss / len(train_dataloader)}\")\n",
    "        \n",
    "        # compute the training accuracy\n",
    "        predictions,classification_accuracy = classify_dataset(model,train_dataloader)\n",
    "        print(f\"Training accuracy: {classification_accuracy*100}%\")\n",
    "        \n",
    "        # compute validation loss for this epoch - could also do this for every batch in one epoch if wanted\n",
    "        running_loss = 0\n",
    "        \n",
    "        for images,labels in val_dataloader: \n",
    "            # get output from updated network\n",
    "            if torch.cuda.is_available():\n",
    "                output = model(images.cuda())\n",
    "            else:\n",
    "                output = model(images)\n",
    "            \n",
    "            # calculate cross-entropy loss for this validation batch\n",
    "            if torch.cuda.is_available():\n",
    "                loss = criterion(output.cuda(),labels.cuda())\n",
    "            else:\n",
    "                loss = criterion(output,labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(f\"Validation loss: {running_loss / len(val_dataloader)}\")\n",
    "            \n",
    "            # compute the training accuracy\n",
    "            predictions,classification_accuracy = classify_dataset(model,val_dataloader)            \n",
    "            print(f\"Validation accuracy: {classification_accuracy*100}%\")\n",
    "            \n",
    "            print(\"===============================================\")\n",
    "            \n",
    "            \n",
    "# save the trained model in a file\n",
    "torch.save(model.state_dict(), 'spinnet_trained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Load our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training model if required\n",
    "if torch.cuda.is_available():\n",
    "    state_dict = torch.load('spinnet_trained_2.7.pth')\n",
    "else:\n",
    "    state_dict = torch.load('spinnet_trained_2.7.pth',map_location=torch.device('cpu'))\n",
    "    \n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) Get classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions,classification_accuracy = classify_dataset(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Classification accuracy: \", str(classification_accuracy * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Plot confusion matrix for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce confusion matrix\n",
    "targets, predictions = get_labels(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(targets, predictions)\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4) meature time for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "image_test,label_test = next(iter(test_dataloader))\n",
    "imshow(image_test[0],label_test[0])\n",
    "start_time = time.time()\n",
    "model_output = model(image_test)\n",
    "print(\"Time taken: \", time.time()-start_time)\n",
    "print(\"Prediction: \", torch.argmax(torch.exp(model_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Real-time webcam demo (does not work on Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = cv2.VideoCapture(0,cv2.CAP_DSHOW) # change to zero usually (I have 2 webcams)\n",
    "vc.set(cv2.CAP_PROP_FRAME_WIDTH, 1280.0)\n",
    "vc.set(cv2.CAP_PROP_FRAME_HEIGHT, 720.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15,15*9/16))\n",
    "\n",
    "for i in range(100):\n",
    "    # get the frame\n",
    "    if vc.isOpened():\n",
    "        is_capturing, frame = vc.read()\n",
    "            \n",
    "    # resize the input image and convert to pytorch tensor\n",
    "    frame_small = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), dsize=(227, 128))\n",
    "    frame_tensor = torch.from_numpy(frame_small.astype(float).transpose(2,0,1)/256).view(1,3,128,227).float()\n",
    "    \n",
    "    # compute CNN output\n",
    "    if torch.cuda.is_available():\n",
    "        output = model(frame_tensor.cuda())\n",
    "    else:\n",
    "        output = model(frame_tensor)\n",
    "        \n",
    "    ps = torch.exp(output)\n",
    "    \n",
    "    # get index of maximum probaability - corresponds to the class\n",
    "    max_val,max_idx = ps[0].max(0)\n",
    "    \n",
    "    # update display\n",
    "    plt.clf()\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) # show the original image (not downscaled version fed to CNN)\n",
    "    plt.axis(\"off\")\n",
    "    if(int(max_idx) == 0):\n",
    "        with plt.rc_context({'axes.titlecolor':'green'}):\n",
    "            plt.title(class_labels[int(max_idx)],fontsize=32)\n",
    "    else:\n",
    "        with plt.rc_context({'axes.titlecolor':'red'}):\n",
    "            plt.title(class_labels[int(max_idx)],fontsize=32)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # fps\n",
    "    time.sleep(1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.release() # switch off the webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
